from __future__ import annotations

import json
from pathlib import Path
from typing import Tuple

import altair as alt
import numpy as np
import pandas as pd
import streamlit as st
from sklearn.metrics import classification_report, confusion_matrix

from spam_classification.train import train as train_fn
from spam_classification.infer import infer_single, load_pipeline
from spam_classification.data import load_dataset, split_dataset


st.set_page_config(page_title="Spam Classification Demo", layout="wide")
st.title("ğŸ“§ Spam Classification â€” Baseline Demo")
st.caption("TF-IDF + LinearSVC baseline with optional calibration; metrics and inference UI.")


ARTIFACTS_DIR = "artifacts"
DATA_DEFAULT = "sms_spam_no_header.csv"


@st.cache_resource
def _load_pipeline_cached(artifacts_dir: str = ARTIFACTS_DIR):
    try:
        return load_pipeline(artifacts_dir)
    except Exception as e:
        st.warning(f"Pipeline æœªè¼‰å…¥ï¼š{e}")
        return None


def _load_metrics(artifacts_dir: str = ARTIFACTS_DIR) -> dict | None:
    p = Path(artifacts_dir) / "metrics.json"
    if not p.exists():
        return None
    try:
        return json.loads(p.read_text())
    except Exception:
        return None


def _plot_confusion(cm: np.ndarray, labels: Tuple[str, str]):
    df_cm = pd.DataFrame(cm, index=labels, columns=labels)
    df_cm = df_cm.reset_index().melt(id_vars="index")
    df_cm.columns = ["True", "Pred", "Count"]
    chart = (
        alt.Chart(df_cm)
        .mark_rect()
        .encode(
            x=alt.X("Pred:N", title="Predicted"),
            y=alt.Y("True:N", title="True"),
            color=alt.Color("Count:Q", scale=alt.Scale(scheme="blues")),
            tooltip=["True", "Pred", "Count"],
        )
    )
    st.altair_chart(chart, use_container_width=True)


def _top_features(pipeline, top_n: int = 20) -> pd.DataFrame | None:
    try:
        tfidf = pipeline.named_steps["tfidf"]
        feature_names = np.array(tfidf.get_feature_names_out())
        clf = pipeline.named_steps["clf"]
        # unwrap calibrated classifier if used
        if hasattr(clf, "estimator"):
            base = clf.estimator
        else:
            base = clf
        # For binary classification, coef_ shape is (1, n_features) or (2, n_features)
        coefs = getattr(base, "coef_", None)
        if coefs is None:
            return None
        # assume positive class is spam; take largest positive weights
        weights = coefs[0] if coefs.ndim == 2 else coefs
        idx = np.argsort(weights)[::-1][:top_n]
        return pd.DataFrame({"feature": feature_names[idx], "weight": weights[idx]})
    except Exception:
        return None


tab = st.sidebar.radio("é¸æ“‡åŠŸèƒ½", ["è¨“ç·´", "æ¨è«–", "æŒ‡æ¨™/è¦–è¦ºåŒ–", "Artifacts"])


if tab == "è¨“ç·´":
    st.subheader("æ¨¡å‹è¨“ç·´")
    data_path = st.text_input("è³‡æ–™æª”æ¡ˆè·¯å¾‘", value=DATA_DEFAULT)
    test_size = st.slider("æ¸¬è©¦é›†æ¯”ä¾‹", 0.1, 0.4, 0.2, 0.05)
    seed = st.number_input("éš¨æ©Ÿç¨®å­", value=42, step=1)
    calibrated = st.checkbox("å•Ÿç”¨æ¦‚ç‡æ ¡æº– (CalibratedClassifierCV)", value=True)
    max_features = st.number_input("TF-IDF æœ€å¤§ç‰¹å¾µæ•¸", value=20000, step=1000)
    run = st.button("é–‹å§‹è¨“ç·´")

    if run:
        if not Path(data_path).exists():
            st.error(f"è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨ï¼š{data_path}")
        else:
            with st.spinner("è¨“ç·´ä¸­ï¼Œè«‹ç¨å€™..."):
                metrics = train_fn(
                    csv_path=data_path,
                    out_dir=ARTIFACTS_DIR,
                    test_size=float(test_size),
                    random_state=int(seed),
                    calibrated=bool(calibrated),
                    max_features=int(max_features),
                )
            st.success("è¨“ç·´å®Œæˆï¼")
            st.json(metrics)


elif tab == "æ¨è«–":
    st.subheader("å–®è¨Šæ¯æ¨è«–")
    message = st.text_area("è¼¸å…¥æ¬²åˆ†é¡çš„è¨Šæ¯æ–‡æœ¬")
    predict = st.button("æ¨è«–")

    if predict:
        pipeline = _load_pipeline_cached(ARTIFACTS_DIR)
        if pipeline is None:
            st.error("å°šæœªæ‰¾åˆ°å·²è¨“ç·´çš„æ¨¡å‹ï¼Œè«‹å…ˆåˆ°ã€è¨“ç·´ã€é é€²è¡Œè¨“ç·´ã€‚")
        else:
            label, conf = infer_single(message, ARTIFACTS_DIR)
            st.write({"label": label, "confidence": round(conf, 4)})


elif tab == "æŒ‡æ¨™/è¦–è¦ºåŒ–":
    st.subheader("è©•ä¼°æŒ‡æ¨™èˆ‡è¦–è¦ºåŒ–")
    metrics = _load_metrics(ARTIFACTS_DIR)
    if not metrics:
        st.warning("å°šæœªæ‰¾åˆ° metrics.jsonï¼Œè«‹å…ˆè‡³ã€è¨“ç·´ã€é åŸ·è¡Œä¸€æ¬¡è¨“ç·´ä»¥ç”¢ç”Ÿè©•ä¼°æŒ‡æ¨™ã€‚")
    else:
        st.metric("Accuracy", f"{metrics['accuracy']:.4f}")
        st.metric("F1 (weighted)", f"{metrics['f1_weighted']:.4f}")

        # ä¾æ“šè¨˜éŒ„çš„ test_size/seed é‡å»ºæ¸¬è©¦é›†ä»¥ç”¢ç”Ÿè¦–è¦ºåŒ–ï¼ˆä½¿ç”¨å·²è¨“ç·´ pipeline é æ¸¬ï¼‰
        csv_path = DATA_DEFAULT
        df = load_dataset(csv_path)
        X_train, X_test, y_train, y_test = split_dataset(
            df, test_size=float(metrics.get("test_size", 0.2)), random_state=int(metrics.get("random_state", 42))
        )
        pipeline = _load_pipeline_cached(ARTIFACTS_DIR)
        if pipeline is None:
            st.error("å°šæœªæ‰¾åˆ°å·²è¨“ç·´çš„æ¨¡å‹ï¼Œè«‹å…ˆåˆ°ã€è¨“ç·´ã€é é€²è¡Œè¨“ç·´ã€‚")
        else:
            preds = pipeline.predict(X_test)
            labels = ("ham", "spam")
            cm = confusion_matrix(y_test, preds, labels=list(labels))
            st.write("Confusion Matrix")
            _plot_confusion(cm, labels)

            st.write("Classification Report (per class)")
            report = classification_report(y_test, preds, labels=list(labels), output_dict=True, zero_division=0)
            df_rep = pd.DataFrame(
                {
                    "label": labels,
                    "precision": [report[l]["precision"] for l in labels],
                    "recall": [report[l]["recall"] for l in labels],
                    "f1": [report[l]["f1-score"] for l in labels],
                    "support": [report[l]["support"] for l in labels],
                }
            )
            st.dataframe(df_rep, use_container_width=True)

            st.write("Top TF-IDF features (by LinearSVC weights)")
            df_top = _top_features(pipeline, top_n=20)
            if df_top is not None:
                st.dataframe(df_top, use_container_width=True)
            else:
                st.info("ç„¡æ³•æ“·å–ç‰¹å¾µæ¬Šé‡ï¼ˆå¯èƒ½ä¸æ”¯æ´æˆ–å°šæœªè¨“ç·´ï¼‰ã€‚")


elif tab == "Artifacts":
    st.subheader("Artifacts æª¢è¦–/ä¸‹è¼‰")
    p = Path(ARTIFACTS_DIR)
    if not p.exists():
        st.warning("å°šæœªç”Ÿæˆ artifactsã€‚")
    else:
        files = list(p.glob("*"))
        st.write("ç¾æœ‰æª”æ¡ˆï¼š", [f.name for f in files])
        # æä¾›ä¸‹è¼‰æŒ‰éˆ•ï¼ˆmetrics.json / model.joblibï¼‰
        m = p / "metrics.json"
        if m.exists():
            st.download_button("ä¸‹è¼‰ metrics.json", data=m.read_text(), file_name="metrics.json")
        model = p / "model.joblib"
        if model.exists():
            st.download_button("ä¸‹è¼‰ model.joblib", data=model.read_bytes(), file_name="model.joblib")

